{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory is : C:\\Users\\karol\\SCI_LEARN\\Github\\Klasyfikator\n",
      "Directory name is : Klasyfikator\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dirpath = os.getcwd()\n",
    "print(\"current directory is : \" + dirpath)\n",
    "foldername = os.path.basename(dirpath)\n",
    "print(\"Directory name is : \" + foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "# Create your connection.\n",
    "cnx = sqlite3.connect('..\\Share\\db\\db.sqlite')\n",
    "\n",
    "df_from_db = pd.read_sql_query(\"SELECT text_review, rating_star FROM reviews\", cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_review</th>\n",
       "      <th>rating_star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [text_review, rating_star]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_db['rating_star'] = df_from_db['rating_star'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "df = df.reindex(np.random.permutation(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./input_data-pl.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./input_data-pl.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "dirpath = os.getcwd()\n",
    "print(\"current directory is : \" + dirpath)\n",
    "foldername = os.path.basename(dirpath)\n",
    "print(\"Directory name is : \" + foldername)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stop = ['a','aby','ach','acz','aczkolwiek','aj','albo','ale','ależ','ani','aż','bardziej','bardzo','bez','bo','bowiem','by','byli','bym','bynajmniej','być','był','była','było','były','będzie','będą','cali','cała','cały','chce','choć','ci','ciebie','cię','co','cokolwiek','coraz','coś','czasami','czasem','czemu','czy','czyli','często','daleko','dla','dlaczego','dlatego','do','dobrze','dokąd','dość','dr','dużo','dwa','dwaj','dwie','dwoje','dzisiaj','dziś','gdy','gdyby','gdyż','gdzie','gdziekolwiek','gdzieś','go','godz','hab','i','ich','ii','iii','ile','im','inna','inne','inny','innych','inż','iv','ix','iż','ja','jak','jakaś','jakby','jaki','jakichś','jakie','jakiś','jakiż','jakkolwiek','jako','jakoś','je','jeden','jedna','jednak','jednakże','jedno','jednym','jedynie','jego','jej','jemu','jest','jestem','jeszcze','jeśli','jeżeli','już','ją','każdy','kiedy','kierunku','kilka','kilku','kimś','kto','ktokolwiek','ktoś','która','które','którego','której','który','których','którym','którzy','ku','lat','lecz','lub','ma','mają','mam','mamy','mało','mgr','mi','miał','mimo','między','mnie','mną','mogą','moi','moim','moja','moje','może','możliwe','można','mu','musi','my','mój','na','nad','nam','nami','nas','nasi','nasz','nasza','nasze','naszego','naszych','natomiast','natychmiast','nawet','nic','nich','nie','niech','niego','niej','niemu','nigdy','nim','nimi','nią','niż','no','nowe','np','nr','o','o.o.','obok','od','ok','około','on','ona','one','oni','ono','oraz','oto','owszem','pan','pana','pani','pl','po','pod','podczas','pomimo','ponad','ponieważ','powinien','powinna','powinni','powinno','poza','prawie','prof','przecież','przed','przede','przedtem','przez','przy','raz','razie','roku','również','sam','sama','się','skąd','sobie','sobą','sposób','swoje','są','ta','tak','taka','taki','takich','takie','także','tam','te','tego','tej','tel','temu','ten','teraz','też','to','tobie','tobą','toteż','totobą','trzeba','tu','tutaj','twoi','twoim','twoja','twoje','twym','twój','ty','tych','tylko','tym','tys','tzw','tę','u','ul','vi','vii','viii','vol','w','wam','wami','was','wasi','wasz','wasza','wasze','we','według','wie','wiele','wielu','więc','więcej','wszyscy','wszystkich','wszystkie','wszystkim','wszystko','wtedy','www','wy','właśnie','wśród','xi','xii','xiii','xiv','xv','z','za','zapewne','zawsze','zaś','ze','zeznowu','znowu','znów','został','zł','żaden','żadna','żadne','żadnych','że','żeby']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    \n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv) # pomija nagłówek\n",
    "        j = 0\n",
    "        for line in csv:\n",
    "            try:\n",
    "                j += 1\n",
    "                text, label = line[:-3], int(line[-2])\n",
    "                #print(text[:60], \"label\", label)\n",
    "            except Exception as e:\n",
    "                #j += 1\n",
    "                #print(\"Linia\",j,e, \"Text:\" ,text, \"label:\", label)\n",
    "\n",
    "                pass\n",
    "       \n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(stream_docs(path='./input_data-pl.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        #print(\"Blad: \", text[:60], \"label\", label)\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore', \n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None, \n",
    "                         tokenizer=tokenizer)\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1)\n",
    "doc_stream = stream_docs(path='./input_data-pl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(300)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(300):\n",
    "    try:\n",
    "        X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "\n",
    "        if not X_train:\n",
    "            break\n",
    "\n",
    "        X_train = vect.transform(X_train)\n",
    "\n",
    "        clf.partial_fit(X_train, y_train, classes=classes)\n",
    "\n",
    "        pbar.update()\n",
    "        \n",
    "    except Exception as e:\n",
    "        \n",
    "        #print(\"Blad:\", e)\n",
    "        pass\n",
    "    \n",
    "    \n",
    "#print(\"======= X_test ========\")\n",
    "#i_train=1\n",
    "#for tst in X_test:\n",
    "#    print(i_train,\")\",tst)\n",
    "#    i_train += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=500)\n",
    "\n",
    "#print(\"======= X_test ========\")\n",
    "#i_test=1\n",
    "#for tst in X_test:\n",
    "#    print(i_test,\")\",tst)\n",
    "#    i_test += 1\n",
    "    \n",
    "#print(\"======= y_test ========\")\n",
    "#i_test=1\n",
    "#for tst in y_test:\n",
    "#    print(i_test,\")\",tst)\n",
    "#    i_test += 1\n",
    "\n",
    "X_test = vect.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Dokładność: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('pkl_objects')\n",
    "\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
    "pickle.dump(clf, open(os.path.join(dest, 'classifier.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./vectorizer.py\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import re\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(\n",
    "                os.path.join(cur_dir, \n",
    "                'pkl_objects', \n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "   \n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current directory is : C:\\Users\\karol\\SCI_LEARN\\Github\\Klasyfikator\n",
      "Directory name is : Klasyfikator\n"
     ]
    }
   ],
   "source": [
    "dirpath = os.getcwd()\n",
    "print(\"current directory is : \" + dirpath)\n",
    "foldername = os.path.basename(dirpath)\n",
    "print(\"Directory name is : \" + foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from vectorizer import vect\n",
    "\n",
    "clf = pickle.load(open(os.path.join('pkl_objects', 'classifier.pkl'), 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "label = {0:'Negatywna', 1:'Pozytywna'}\n",
    "\n",
    "example = ['Sklep jest super']\n",
    "X = vect.transform(example)\n",
    "print('Prognoza: %s\\nPrawdopodobieństwo: %.2f%%' %\\\n",
    "      (label[clf.predict(X)[0]], clf.predict_proba(X).max()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
